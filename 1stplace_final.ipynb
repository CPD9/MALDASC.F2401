{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304d9dc2-83a8-426e-b252-6cc437682b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q kaggle\n",
    "# !mkdir -p ./kaggle\n",
    "# !cp kaggle.json ./kaggle/\n",
    "# !chmod 600 ./kaggle/kaggle.json\n",
    "# !cat ./kaggle/kaggle.json\n",
    "# ! kaggle datasets list\n",
    "# !kaggle datasets download -d haqishen/timm-20220211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12eb849d-02b9-4c7d-a019-14c47354ee2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !unzip ./timm-20220211.zip -d ./timm-20220211\n",
    "# !cp -r ./timm-20220211/pytorch-image-models-master/timm/ ./timm4smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d469b59-c4d2-4e5d-b5f8-c7ab319e01ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/conda/lib/python3.11/site-packages (0.9.2)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.11/site-packages (from timm) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from timm) (0.16.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from timm) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.7->timm) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.7->timm) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.7->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.7->timm) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->timm) (4.66.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: albumentations in /opt/conda/lib/python3.11/site-packages (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.11/site-packages (from albumentations) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (1.11.3)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (0.23.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (4.11.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /opt/conda/lib/python3.11/site-packages (from albumentations) (1.4.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (2.7.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (4.9.0.80)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations) (2.18.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (3.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (10.0.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (2024.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (23.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.16.0)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (0.9.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (4.66.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from segmentation_models_pytorch) (10.0.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.11/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: monai in /opt/conda/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.11/site-packages (from monai) (1.26.0)\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.11/site-packages (from monai) (2.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->monai) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->monai) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->monai) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->monai) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->monai) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.9->monai) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: imageio in /opt/conda/lib/python3.11/site-packages (2.34.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from imageio) (1.26.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio) (10.0.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pydicom in /opt/conda/lib/python3.11/site-packages (2.4.4)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nibabel in /opt/conda/lib/python3.11/site-packages (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.11/site-packages (from nibabel) (1.26.0)\n",
      "Requirement already satisfied: packaging>=17 in /opt/conda/lib/python3.11/site-packages (from nibabel) (23.1)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: cv in /opt/conda/lib/python3.11/site-packages (1.0.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pylibjpeg==1.4.0 in /opt/conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from pylibjpeg==1.4.0) (1.26.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python_gdcm==3.0.17.1 in /opt/conda/lib/python3.11/site-packages (3.0.17.1)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pylibjpeg-libjpeg in /opt/conda/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.24 in /opt/conda/lib/python3.11/site-packages (from pylibjpeg-libjpeg) (1.26.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install albumentations\n",
    "!pip install segmentation_models_pytorch\n",
    "!pip install monai\n",
    "!pip install imageio\n",
    "!pip install pydicom\n",
    "!pip install pandas\n",
    "!pip install nibabel\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install cv\n",
    "!pip install pylibjpeg==1.4.0\n",
    "!pip install python_gdcm==3.0.17.1\n",
    "!pip install pylibjpeg-libjpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d853972-3b8b-44f6-bfb6-7eac38e4cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.9.2', '0.5.5')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "import timm\n",
    "import timm4smp\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "timm.__version__, timm4smp.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753f139a-4c6d-49c9-9e2e-c634bac4f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Dataset'\n",
    "image_size_seg = (128, 128, 128)\n",
    "msk_size = image_size_seg[0]\n",
    "image_size_cls = 224\n",
    "n_slice_per_c = 15\n",
    "n_ch = 5\n",
    "\n",
    "batch_size_seg = 1\n",
    "num_workers = 2\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8d231e-4af2-4fc3-a714-ae09207acd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>prediction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.10197_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10197</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.10454_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10454</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.10690_C1</td>\n",
       "      <td>1.2.826.0.1.3680043.10690</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         row_id           StudyInstanceUID prediction_type\n",
       "0  1.2.826.0.1.3680043.10197_C1  1.2.826.0.1.3680043.10197              C1\n",
       "1  1.2.826.0.1.3680043.10454_C1  1.2.826.0.1.3680043.10454              C1\n",
       "2  1.2.826.0.1.3680043.10690_C1  1.2.826.0.1.3680043.10690              C1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./Dataset/test.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe1b775-07b3-4438-8493-40b3ba746756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>image_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.22327</td>\n",
       "      <td>./Dataset/test_images/1.2.826.0.1.3680043.22327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.25399</td>\n",
       "      <td>./Dataset/test_images/1.2.826.0.1.3680043.25399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.5876</td>\n",
       "      <td>./Dataset/test_images/1.2.826.0.1.3680043.5876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            StudyInstanceUID                                     image_folder\n",
       "0  1.2.826.0.1.3680043.22327  ./Dataset/test_images/1.2.826.0.1.3680043.22327\n",
       "1  1.2.826.0.1.3680043.25399  ./Dataset/test_images/1.2.826.0.1.3680043.25399\n",
       "2   1.2.826.0.1.3680043.5876   ./Dataset/test_images/1.2.826.0.1.3680043.5876"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'train.csv')).head(1500)\n",
    "    df = pd.DataFrame({\n",
    "        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n",
    "    })\n",
    "    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
    "else:\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    if df.iloc[0].row_id == '1.2.826.0.1.3680043.10197_C1':\n",
    "        # test_images and test.csv are inconsistent in the dev dataset, fixing labels for the dev run.\n",
    "        df = pd.DataFrame({\n",
    "            \"row_id\": ['1.2.826.0.1.3680043.22327_C1', '1.2.826.0.1.3680043.25399_C1', '1.2.826.0.1.3680043.5876_C1'],\n",
    "            \"StudyInstanceUID\": ['1.2.826.0.1.3680043.22327', '1.2.826.0.1.3680043.25399', '1.2.826.0.1.3680043.5876'],\n",
    "            \"prediction_type\": [\"C1\", \"C1\", \"patient_overall\"]}\n",
    "        )\n",
    "    df = pd.DataFrame({\n",
    "        'StudyInstanceUID': df['StudyInstanceUID'].unique().tolist()\n",
    "    })\n",
    "    df['image_folder'] = df['StudyInstanceUID'].apply(lambda x: os.path.join(data_dir, 'test_images', x))\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad507dc-b8ef-4070-bd2b-28f6c2cdf20c",
   "metadata": {},
   "source": [
    "<h2>Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a1f1ab-14ef-4044-b510-75b970ceac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = cv2.resize(data, (image_size_seg[0], image_size_seg[1]), interpolation = cv2.INTER_AREA)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dicom_line_par(path):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(path, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "\n",
    "    n_scans = len(t_paths)\n",
    "#     print(n_scans)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., image_size_seg[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "\n",
    "    images = []\n",
    "    for filename in t_paths:\n",
    "        images.append(load_dicom(filename))\n",
    "    images = np.stack(images, -1)\n",
    "    \n",
    "    images = images - np.min(images)\n",
    "    images = images / (np.max(images) + 1e-4)\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class SegTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        image = load_dicom_line_par(row.image_folder)\n",
    "        if image.ndim < 4:\n",
    "            image = np.expand_dims(image, 0)\n",
    "        image = image.astype(np.float32).repeat(3, 0)  # to 3ch\n",
    "        image = image / 255.\n",
    "        return torch.tensor(image).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd432f2a-a490-4610-aa9f-79be81027800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seg = SegTestDataset(df)\n",
    "loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80649d42-5649-4208-819c-196348bc871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rcParams['figure.figsize'] = 20,8\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,4)\n",
    "        for p in range(4):\n",
    "            idx = i*4+p\n",
    "            img = dataset_seg[idx]\n",
    "            img = img[:, :, :, 60]\n",
    "            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125ffcfa-757d-4769-8a85-56adbca5886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rcParams['figure.figsize'] = 20,8\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,4)\n",
    "        for p in range(4):\n",
    "            idx = i*4+p\n",
    "            img = dataset_seg[idx]\n",
    "            img = img[:, :, 60, :]\n",
    "            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e18e651-83b7-4e7a-8c2b-18688915b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    rcParams['figure.figsize'] = 20,8\n",
    "    for i in range(2):\n",
    "        f, axarr = plt.subplots(1,4)\n",
    "        for p in range(4):\n",
    "            idx = i*4+p\n",
    "            img = dataset_seg[idx]\n",
    "            img = img[:, 60, :, :]\n",
    "            axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d890662-7ddb-4b79-9a68-5c50000f79bd",
   "metadata": {},
   "source": [
    "<h2>Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1877f514-c0fe-4cb4-bca3-8480a94f156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Conv3d w/ Same Padding\n",
    "modified from:\n",
    "https://github.com/rwightman/pytorch-image-models/blob/a2727c1bf78ba0d7b5727f5f95e37fb7f8866b1f/timm/models/layers/conv2d_same.py\n",
    "https://github.com/rwightman/pytorch-image-models/blob/a2727c1bf78ba0d7b5727f5f95e37fb7f8866b1f/timm/models/layers/padding.py\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "\n",
    "# Calculate symmetric padding for a convolution\n",
    "def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n",
    "    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n",
    "    return padding\n",
    "\n",
    "\n",
    "# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\n",
    "def get_same_padding(x: int, k: int, s: int, d: int):\n",
    "    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
    "\n",
    "\n",
    "# Can SAME padding for given args be done statically?\n",
    "def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):\n",
    "    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n",
    "\n",
    "\n",
    "# Dynamically pad input x with 'SAME' padding for conv with specified args\n",
    "def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1, 1), value: float = 0):\n",
    "    ih, iw, iz = x.size()[-3:]\n",
    "    pad_h = get_same_padding(ih, k[0], s[0], d[0])\n",
    "    pad_w = get_same_padding(iw, k[1], s[1], d[1])\n",
    "    pad_z = get_same_padding(iz, k[2], s[2], d[2])\n",
    "    if pad_h > 0 or pad_w > 0 or pad_z > 0:\n",
    "        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2, pad_z // 2, pad_z - pad_z // 2], value=value)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:\n",
    "    dynamic = False\n",
    "    if isinstance(padding, str):\n",
    "        # for any string padding, the padding will be calculated for you, one of three ways\n",
    "        padding = padding.lower()\n",
    "        if padding == 'same':\n",
    "            # TF compatible 'SAME' padding, has a performance and GPU memory allocation impact\n",
    "            if is_static_pad(kernel_size, **kwargs):\n",
    "                # static case, no extra overhead\n",
    "                padding = get_padding(kernel_size, **kwargs)\n",
    "            else:\n",
    "                # dynamic 'SAME' padding, has runtime/GPU memory overhead\n",
    "                padding = 0\n",
    "                dynamic = True\n",
    "        elif padding == 'valid':\n",
    "            # 'VALID' padding, same as padding=0\n",
    "            padding = 0\n",
    "        else:\n",
    "            # Default to PyTorch style 'same'-ish symmetric padding\n",
    "            padding = get_padding(kernel_size, **kwargs)\n",
    "    return padding, dynamic\n",
    "\n",
    "\n",
    "def conv3d_same(\n",
    "        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int, int] = (1, 1, 1),\n",
    "        padding: Tuple[int, int, int] = (0, 0, 0), dilation: Tuple[int, int, int] = (1, 1, 1), groups: int = 1):\n",
    "    x = pad_same(x, weight.shape[-3:], stride, dilation)\n",
    "    return F.conv3d(x, weight, bias, stride, (0, 0, 0), dilation, groups)\n",
    "\n",
    "\n",
    "class Conv3dSame(nn.Conv3d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3d convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def create_conv3d_pad(in_chs, out_chs, kernel_size, **kwargs):\n",
    "    padding = kwargs.pop('padding', '')\n",
    "    kwargs.setdefault('bias', False)\n",
    "    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n",
    "    if is_dynamic:\n",
    "        return Conv3dSame(in_chs, out_chs, kernel_size, **kwargs)\n",
    "    else:\n",
    "        return nn.Conv3d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86bc4b56-b81b-4a2a-a1d0-3397f047b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "\n",
    "\n",
    "class PAB(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pab_channels=64):\n",
    "        super(PAB, self).__init__()\n",
    "        # Series of 1x1 conv to generate attention feature maps\n",
    "        self.pab_channels = pab_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "        self.center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "        self.bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.map_softmax = nn.Softmax(dim=1)\n",
    "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsize = x.size()[0]\n",
    "        h = x.size()[2]\n",
    "        w = x.size()[3]\n",
    "        d = x.size()[4]\n",
    "        x_top = self.top_conv(x)\n",
    "        x_center = self.center_conv(x)\n",
    "        x_bottom = self.bottom_conv(x)\n",
    "\n",
    "        x_top = x_top.flatten(2)\n",
    "        x_center = x_center.flatten(2).transpose(1, 2)\n",
    "        x_bottom = x_bottom.flatten(2).transpose(1, 2)\n",
    "        sp_map = torch.matmul(x_center, x_top)\n",
    "        sp_map = self.map_softmax(sp_map.view(bsize, -1)).view(bsize, h*w*d, h*w*d)\n",
    "        sp_map = torch.matmul(sp_map, x_bottom)\n",
    "        sp_map = sp_map.reshape(bsize, self.in_channels, h, w, d)\n",
    "        x = x + sp_map\n",
    "        x = self.out_conv(x)\n",
    "        # print('x_top',x_top.shape,'x_center',x_center.shape,'x_bottom',x_bottom.shape,'x',x.shape,'sp_map',sp_map.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MFAB(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True, reduction=16):\n",
    "        # MFAB is just a modified version of SE-blocks, one for skip, one for input\n",
    "        super(MFAB, self).__init__()\n",
    "        self.hl_conv = nn.Sequential(\n",
    "            md.Conv2dReLU(\n",
    "                in_channels,\n",
    "                in_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                use_batchnorm=use_batchnorm,\n",
    "            ),\n",
    "            md.Conv2dReLU(\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                kernel_size=1,\n",
    "                use_batchnorm=use_batchnorm,\n",
    "            ),\n",
    "        )\n",
    "        reduced_channels = max(1, skip_channels // reduction)\n",
    "        self.SE_ll = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(skip_channels, reduced_channels, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(reduced_channels, skip_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.SE_hl = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(skip_channels, reduced_channels, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(reduced_channels, skip_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            skip_channels + skip_channels,  # we transform C-prime form high level to C from skip connection\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.hl_conv(x)\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        attention_hl = self.SE_hl(x)\n",
    "        if skip is not None:\n",
    "            attention_ll = self.SE_ll(skip)\n",
    "            attention_hl = attention_hl + attention_ll\n",
    "            x = x * attention_hl\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels, use_batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MAnetDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels,\n",
    "        n_blocks=5,\n",
    "        reduction=16,\n",
    "        use_batchnorm=True,\n",
    "        pab_channels=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.center = PAB(head_channels, head_channels, pab_channels=pab_channels)\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(use_batchnorm=use_batchnorm)  # no attention type here\n",
    "        blocks = [\n",
    "            MFAB(in_ch, skip_ch, out_ch, reduction=reduction, **kwargs)\n",
    "            if skip_ch > 0\n",
    "            else DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        # for the last we dont have skip connection -> use simple decoder block\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, *features):\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11125ffd-e4e2-4a87-9a80-06bcc75967d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm4smp.models.layers.conv2d_same import Conv2dSame\n",
    "from timm4smp.models import create_model\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='MAnet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm4smp.models.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        # num_input_channels = saved_model['encoder.bn1.running_mean'].shape[0]\n",
    "        # self.encoder.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'MAnet':\n",
    "            self.decoder =  MAnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, backbone, image_size, pretrained=False):\n",
    "        super(TimmModel, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=0,\n",
    "            drop_path_rate=0,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone or 'nfnet' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * 7, in_chans, self.image_size, self.image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * 7, -1)\n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = feat1.contiguous().view(bs * n_slice_per_c * 7, 512)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "\n",
    "        return self.head(feat1), self.head2(feat2[:, 0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Timm1BoneModel(nn.Module):\n",
    "    def __init__(self, backbone, image_size, pretrained=False):\n",
    "        super(Timm1BoneModel, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=0,\n",
    "            drop_path_rate=0,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone or 'nfnet' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c, in_chans, self.image_size, self.image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c, -1)\n",
    "        feat, _ = self.lstm(feat)\n",
    "        feat = feat.contiguous().view(bs * n_slice_per_c, -1)\n",
    "        feat = self.head(feat)\n",
    "        feat = feat.view(bs, n_slice_per_c).contiguous()\n",
    "\n",
    "        return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293b2c6-7f46-4c47-ba00-92f5e253d60a",
   "metadata": {},
   "source": [
    "<h2>Load Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d960514-fdcb-489b-b9cc-4189d6e16f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_seg = []\n",
    "\n",
    "kernel_type = 'timm3d_manet_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "backbone = 'resnet18d'\n",
    "model_dir_seg = 'my_models'\n",
    "n_blocks = 4\n",
    "for fold in range(5):\n",
    "    model = TimmSegModel(backbone, pretrained=False)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    # Assuming `model` is your PyTorch model\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"Parameter name: {name}, Size: {param.size()}\")\n",
    "\n",
    "    model.load_state_dict(sd, strict=False)\n",
    "    model.eval()\n",
    "    models_seg.append(model)\n",
    "\n",
    "len(models_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf87694-d725-494c-aa65-5530d8c85527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnetv2_s_in21ft1k to current tf_efficientnetv2_s.in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep'\n",
    "model_dir_cls = './models'\n",
    "backbone = 'tf_efficientnetv2_s_in21ft1k'\n",
    "in_chans = 6\n",
    "models_cls1 = []\n",
    "\n",
    "for fold in range(5):\n",
    "    model = Timm1BoneModel(backbone, image_size=224, pretrained=False)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file, map_location='cpu')\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    models_cls1.append(model)\n",
    "\n",
    "len(models_cls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a3cf89-dea6-433b-a27c-b34feb8c0825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_type = '0920_2d_lstmv22headv2_convnn_224_15_6ch_8flip_augv2_drl3_rov1p2_rov3p2_bs4_lr6e5_eta6e6_lw151_50ep'\n",
    "model_dir_cls = './models'\n",
    "backbone = 'convnext_nano'\n",
    "in_chans = 6\n",
    "models_cls2 = []\n",
    "\n",
    "for fold in range(5):\n",
    "    model = TimmModel(backbone, image_size=224, pretrained=False)\n",
    "    model = model.to(device)\n",
    "    load_model_file = os.path.join(model_dir_cls, f'{kernel_type}_fold{fold}_best.pth')\n",
    "    sd = torch.load(load_model_file)\n",
    "    if 'model_state_dict' in sd.keys():\n",
    "        sd = sd['model_state_dict']\n",
    "    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    models_cls2.append(model)\n",
    "\n",
    "len(models_cls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91bd6782-a7d9-4f9a-91d3-aa9f8b0bf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "266997af-3f6d-4246-b6c6-14d6e605ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bone(msk, cid, t_paths, cropped_images):\n",
    "    n_scans = len(t_paths)\n",
    "    bone = []\n",
    "    try:\n",
    "        msk_b = msk[cid] > 0.2\n",
    "        msk_c = msk[cid] > 0.05\n",
    "\n",
    "        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
    "        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n",
    "        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
    "            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n",
    "            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
    "            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n",
    "        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n",
    "        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n",
    "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n",
    "\n",
    "        inds = np.linspace(zz1 ,zz2-1 ,n_slice_per_c).astype(int)\n",
    "        inds_ = np.linspace(z1 ,z2-1 ,n_slice_per_c).astype(int)\n",
    "        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n",
    "\n",
    "            msk_this = msk[cid, :, :, ind_]\n",
    "\n",
    "            images = []\n",
    "            for i in range(-n_ch//2+1, n_ch//2+1):\n",
    "                try:\n",
    "                    dicom = pydicom.read_file(t_paths[ind+i])\n",
    "                    images.append(dicom.pixel_array)\n",
    "                except:\n",
    "                    images.append(np.zeros((512, 512)))\n",
    "\n",
    "            data = np.stack(images, -1)\n",
    "            data = data - np.min(data)\n",
    "            data = data / (np.max(data) + 1e-4)\n",
    "            data = (data * 255).astype(np.uint8)\n",
    "            msk_this = msk_this[x1:x2, y1:y2]\n",
    "            xx1 = int(x1 / msk_size * data.shape[0])\n",
    "            xx2 = int(x2 / msk_size * data.shape[0])\n",
    "            yy1 = int(y1 / msk_size * data.shape[1])\n",
    "            yy2 = int(y2 / msk_size * data.shape[1])\n",
    "            data = data[xx1:xx2, yy1:yy2]\n",
    "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
    "            msk_this = (msk_this * 255).astype(np.uint8)\n",
    "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "            data = np.concatenate([data, msk_this[:, :, np.newaxis]], -1)\n",
    "\n",
    "            bone.append(torch.tensor(data))\n",
    "\n",
    "    except:\n",
    "        for sid in range(n_slice_per_c):\n",
    "            bone.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n",
    "\n",
    "    cropped_images[cid] = torch.stack(bone, 0)\n",
    "\n",
    "\n",
    "def load_cropped_images(msk, image_folder, n_ch=n_ch):\n",
    "\n",
    "    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "    for cid in range(7):\n",
    "        threads[cid] = threading.Thread(target=load_bone, args=(msk, cid, t_paths, cropped_images))\n",
    "        threads[cid].start()\n",
    "    for cid in range(7):\n",
    "        threads[cid].join()\n",
    "\n",
    "    return torch.cat(cropped_images, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa563b-78a0-4969-a0cb-6efe2e879e86",
   "metadata": {},
   "source": [
    "<h2>Predict</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c1df18-fc88-4d8c-a532-40c42f69b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [01:58<00:00, 39.58s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs1 = []\n",
    "outputs2 = []\n",
    "\n",
    "bar = tqdm(loader_seg)\n",
    "with torch.no_grad():\n",
    "    for batch_id, (images) in enumerate(bar):\n",
    "        images = images.cuda()\n",
    "\n",
    "        # SEG\n",
    "        pred_masks = []\n",
    "        for model in models_seg:\n",
    "            pmask = model(images).sigmoid()\n",
    "            pred_masks.append(pmask)\n",
    "        pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n",
    "\n",
    "        # Build cls input\n",
    "        cls_inp = []\n",
    "        threads = [None] * 7\n",
    "        cropped_images = [None] * 7\n",
    "\n",
    "        for i in range(pred_masks.shape[0]):\n",
    "            row = df.iloc[batch_id*batch_size_seg+i]\n",
    "            cropped_images = load_cropped_images(pred_masks[i], row.image_folder)\n",
    "            cls_inp.append(cropped_images.permute(0, 3, 1, 2).float() / 255.)\n",
    "        cls_inp = torch.stack(cls_inp, 0).to(device)  # (1, 105, 6, 224, 224)\n",
    "\n",
    "        pred_cls1, pred_cls2 = [], []\n",
    "        # CLS 2\n",
    "        for _, model in enumerate(models_cls2):\n",
    "            logits, logits2 = model(cls_inp)\n",
    "            pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "            pred_cls2.append(logits2.sigmoid())\n",
    "\n",
    "        # CLS 1\n",
    "        cls_inp = cls_inp.view(7, 15, 6, image_size_cls, image_size_cls).contiguous()\n",
    "        for _, model in enumerate(models_cls1):\n",
    "            logits = model(cls_inp)\n",
    "            pred_cls1.append(logits.sigmoid().view(-1, 7, n_slice_per_c))\n",
    "\n",
    "        pred_cls1 = torch.stack(pred_cls1, 0).mean(0)\n",
    "        pred_cls2 = torch.stack(pred_cls2, 0).mean(0)\n",
    "        outputs1.append(pred_cls1.cpu())\n",
    "        outputs2.append(pred_cls2.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b5d2c-fa95-4c98-a9d0-fa37406b7ee3",
   "metadata": {},
   "source": [
    "<h2>Outputs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4a88052-a12d-4d52-855a-a4261a298b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1 = torch.cat(outputs1)\n",
    "outputs2 = torch.cat(outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef86c619-a505-446f-b13e-97786678b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED1 = (outputs1.mean(-1)).clamp(0.0001, 0.9999)\n",
    "PRED2 = (outputs2.view(-1)).clamp(0.0001, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac219fac-dbe7-4352-81c2-2b32cb6b5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ids = []\n",
    "for _, row in df.iterrows():\n",
    "    for i in range(7):\n",
    "        row_ids.append(row.StudyInstanceUID + f'_C{i+1}')\n",
    "    row_ids.append(row.StudyInstanceUID + '_patient_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c28c4c0-e7db-4b14-8b79-956bb573db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "    'row_id': row_ids,\n",
    "    'fractured': torch.cat([PRED1, PRED2.unsqueeze(1)], 1).view(-1),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abfcf21a-d1ef-468c-a8f3-afdf790e187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submissionmanet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccaa0867-ddc0-4aba-b3d7-69bafb5f12b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>fractured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C1</td>\n",
       "      <td>0.096731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C2</td>\n",
       "      <td>0.128058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C3</td>\n",
       "      <td>0.104043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C4</td>\n",
       "      <td>0.100579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C5</td>\n",
       "      <td>0.109479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C6</td>\n",
       "      <td>0.182498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_C7</td>\n",
       "      <td>0.238201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.2.826.0.1.3680043.22327_patient_overall</td>\n",
       "      <td>0.562196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C1</td>\n",
       "      <td>0.093270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C2</td>\n",
       "      <td>0.135869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C3</td>\n",
       "      <td>0.100735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C4</td>\n",
       "      <td>0.102687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C5</td>\n",
       "      <td>0.097606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C6</td>\n",
       "      <td>0.163421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_C7</td>\n",
       "      <td>0.192874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.2.826.0.1.3680043.25399_patient_overall</td>\n",
       "      <td>0.561975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C1</td>\n",
       "      <td>0.098407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C2</td>\n",
       "      <td>0.168433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C3</td>\n",
       "      <td>0.115814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C4</td>\n",
       "      <td>0.112480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C5</td>\n",
       "      <td>0.126477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C6</td>\n",
       "      <td>0.225208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_C7</td>\n",
       "      <td>0.270872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.2.826.0.1.3680043.5876_patient_overall</td>\n",
       "      <td>0.561953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       row_id  fractured\n",
       "0                1.2.826.0.1.3680043.22327_C1   0.096731\n",
       "1                1.2.826.0.1.3680043.22327_C2   0.128058\n",
       "2                1.2.826.0.1.3680043.22327_C3   0.104043\n",
       "3                1.2.826.0.1.3680043.22327_C4   0.100579\n",
       "4                1.2.826.0.1.3680043.22327_C5   0.109479\n",
       "5                1.2.826.0.1.3680043.22327_C6   0.182498\n",
       "6                1.2.826.0.1.3680043.22327_C7   0.238201\n",
       "7   1.2.826.0.1.3680043.22327_patient_overall   0.562196\n",
       "8                1.2.826.0.1.3680043.25399_C1   0.093270\n",
       "9                1.2.826.0.1.3680043.25399_C2   0.135869\n",
       "10               1.2.826.0.1.3680043.25399_C3   0.100735\n",
       "11               1.2.826.0.1.3680043.25399_C4   0.102687\n",
       "12               1.2.826.0.1.3680043.25399_C5   0.097606\n",
       "13               1.2.826.0.1.3680043.25399_C6   0.163421\n",
       "14               1.2.826.0.1.3680043.25399_C7   0.192874\n",
       "15  1.2.826.0.1.3680043.25399_patient_overall   0.561975\n",
       "16                1.2.826.0.1.3680043.5876_C1   0.098407\n",
       "17                1.2.826.0.1.3680043.5876_C2   0.168433\n",
       "18                1.2.826.0.1.3680043.5876_C3   0.115814\n",
       "19                1.2.826.0.1.3680043.5876_C4   0.112480\n",
       "20                1.2.826.0.1.3680043.5876_C5   0.126477\n",
       "21                1.2.826.0.1.3680043.5876_C6   0.225208\n",
       "22                1.2.826.0.1.3680043.5876_C7   0.270872\n",
       "23   1.2.826.0.1.3680043.5876_patient_overall   0.561953"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20d748-ed00-491d-ba04-40c676a26e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
